
##  Papers LLms
- **A Platform for the Biomedical Application of Large Language Models** â€” [Nature](https://www.nature.com/articles/s41587-024-02534-3)
- **Titans: Learning to Memorize at Test Time** â€” [arXiv:2501.00663](https://arxiv.org/abs/2501.00663)
- **Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture (JEPA)** â€” [arXiv:2301.08243](https://arxiv.org/abs/2301.08243)
- **A Survey of Context Engineering for Large Language Models** â€” [arXiv:2507.13334](https://arxiv.org/abs/2507.13334)

- **Reinforcement Learning for Reasoning in Large Language Models with One Training Example** â€” [arXiv:2504.20571](https://arxiv.org/abs/2504.20571)  
- **A Survey on Inference Engines for Large Language Models: Perspectives on Optimization and Efficiency** â€” [arXiv:2505.01658](https://arxiv.org/abs/2505.01658)  
- **A Survey on Multimodal Large Language Models** â€” [arXiv:2306.13549](https://arxiv.org/abs/2306.13549)  
- **DISTILLM-2: A Contrastive Approach Boosts the Distillation of LLMs** â€” [arXiv:2503.07067](https://arxiv.org/abs/2503.07067)  
- **Insights into DeepSeek-V3: Scaling Challenges and Reflections on Hardware for AI Architectures** â€” [arXiv:2505.09343](https://arxiv.org/abs/2505.09343)  
- **Reward Reasoning Model** â€” [arXiv:2505.14674](https://arxiv.org/abs/2505.14674)  
- **Skywork-VL Reward: An Effective Reward Model for Multimodal Understanding and Reasoning** â€” [arXiv:2505.07263 (PDF)](https://arxiv.org/pdf/2505.07263)  
- **Unified Multimodal Chain-of-Thought Reward Model through Reinforcement Fine-Tuning** â€” [arXiv:2505.03318](https://arxiv.org/abs/2505.03318)  
- **Unifying Large Language Models and Knowledge Graphs: A Roadmap** â€” [arXiv:2306.08302 (PDF)](https://arxiv.org/pdf/2306.08302)  

## Knowledge Distillation

- **Module-wise Adaptive Distillation for Multimodality Foundation Models** â€” [arXiv:2310.04550](https://arxiv.org/abs/2310.04550)  
- **Unlock the Power: Competitive Distillation for Multi-Modal Large Language Models** â€” [arXiv:2311.08213](https://arxiv.org/abs/2311.08213)  
- **LoRA-Enhanced Distillation on Guided Diffusion Models** â€” [arXiv:2312.06899](https://arxiv.org/abs/2312.06899)  
- **Multi Teacher Privileged Knowledge Distillation for Multimodal Expression Recognition** â€” [arXiv:2408.09035 (PDF)](https://arxiv.org/pdf/2408.09035)  


  
##  Multimodal Model Research

- **BLIP3-o: A Family of Fully Open Unified Multimodal Models â€” Architecture, Training, and Dataset** â€” [arXiv:2505.09568](https://arxiv.org/abs/2505.09568)  
- **Emerging Properties in Unified Multimodal Pretraining** â€” [arXiv:2505.14683](https://arxiv.org/abs/2505.14683)  
- **MMaDA: Multimodal Large Diffusion Language Models** â€” [arXiv:2505.15809](https://arxiv.org/abs/2505.15809)  
- **When Continue Learning Meets Multimodal Large Language Model: A Survey** â€” [arXiv:2503.01887](https://arxiv.org/abs/2503.01887)  


##  Small Language Model Research

- **Teaching Small Language Models Reasoning through Counterfactual Distillation** â€” [arXiv:2212.08410](https://arxiv.org/abs/2212.08410)  
- **Efficient Knowledge Distillation: Empowering Small Language Models with Teacher Model Insights** â€” [arXiv:2409.12586 (PDF)](https://arxiv.org/pdf/2409.12586)  
- **Small Language Models are the Future of Agentic AI** â€” [arXiv:2506.02153](https://arxiv.org/abs/2506.02153)  


##  Videos
- **Likelihood** â€” [YouTube](https://www.youtube.com/watch?v=-eGJuwQ5A2o&t=479s)

##  Courses
- **YSDA Natural Language Processing Course** â€” [GitHub](https://github.com/yandexdataschool/nlp_course)

##  Useful Resources

- **500+ AI Agent Projects / Use Cases** â€” [GitHub Repository](https://github.com/ashishpatel26/500-AI-Agents-Projects?tab=readme-ov-file)

- **Nir Diamant â€“ Generativeâ€¯AI â€¢ LLM â€¢ RAG â€¢ Production Agents** - [Github](https://github.com/NirDiamant)



- **RAGFlow** â€” An open-source RAG engine based on deep document understanding, combining LLMs for truthful QA with well-founded citations from complex formatted data.  
  [GitHub](https://github.com/infiniflow/ragflow)

- **nano-graphrag** â€” A smaller, faster, cleaner GraphRAG implementation. Portable, asynchronous, fully typed, and easier to hack than the official Microsoft version.  
  [GitHub](https://github.com/gusye1234/nano-graphrag)

- **GraphRAG-Local-UI** â€” A local adaptation of Microsoftâ€™s GraphRAG supporting local models, with a comprehensive interactive UI ecosystem for indexing, prompt tuning, querying, and chatting.  
  [GitHub](https://github.com/severian42/GraphRAG-Local-UI)

- **LLM Graph Builder** â€” Transforms unstructured data (PDF, DOC, TXT, YouTube, web pages) into a structured Knowledge Graph stored in Neo4j using LLMs and LangChain.  
  [GitHub](https://github.com/neo4j-labs/llm-graph-builder)

- **CAMEL** â€” An open-source community focused on studying the scaling laws of agents, supporting diverse agents, tasks, prompts, models, and simulated environments.  
  [GitHub](https://github.com/camel-ai/camel)

- **FlashRAG** â€” Python toolkit for RAG research, including 36 benchmark datasets and 23 state-of-the-art RAG algorithms, including reasoning-based methods.  
  [GitHub](https://github.com/RUC-NLPIR/FlashRAG)

- **kotaemon** â€” Clean and customizable open-source RAG UI for chatting with your documents, built for both end users and developers.  
  [GitHub](https://github.com/Cinnamon/kotaemon)

- **DB-GPT** â€” AI-native data app framework with agentic workflow orchestration, multi-model management, RAG and Multi-Agents frameworks for simplifying large model applications with data.  
  [Link](https://chatgpt.com/c/6896df09-83a8-8320-bd78-9624c542f703)

- **GraphRAG** â€” Microsoftâ€™s data pipeline and transformation suite that extracts structured data from unstructured text using LLMs to enhance reasoning on private data.  
  [GitHub](https://github.com/microsoft/graphrag)

- **HippoRAG 2** â€” Memory framework for LLMs improving their ability to recognize and utilize connections in new knowledge, mimicking human long-term memory.  
  [GitHub](https://github.com/OSU-NLP-Group/HippoRAG)


##  Books
- **AI Engineering: Building Applications with Foundation Models** â€” Chip Huyen â€” [Amazon](https://www.amazon.com/AI-Engineering-Building-Applications-Foundation/dp/1098166302)
- **Designing Machine Learning Systems: An Iterative Process for Production-Ready Applications** â€” Chip Huyen â€” [Amazon](https://www.amazon.co.uk/Designing-Machine-Learning-Systems-Production-Ready/dp/1098107969)
- **Hands-On Large Language Models: Language Understanding and Generation** â€” Jay Alammar, Maarten Grootendorst â€” [Amazon](https://www.amazon.co.uk/Hands-Large-Language-Models-Understanding-ebook/dp/B0DGZ46G88) [Code](https://github.com/HandsOnLLM/Hands-On-Large-Language-Models?tab=readme-ov-file)
- **Practical MLOps: Operationalizing Machine Learning Models** â€” Noah Gift, Alfredo Deza â€” [Amazon](https://www.amazon.co.uk/Practical-MLOps-Operationalizing-Machine-Learning/dp/1098103017)
- **Introduction to Machine Learning Interviews Book** â€” Chip Huyen -- [https://huyenchip.com/ml-interviews-book/]
- **ğ—•ğ˜‚ğ—¶ğ—¹ğ—± ğ—® ğ—Ÿğ—®ğ—¿ğ—´ğ—² ğ—Ÿğ—®ğ—»ğ—´ğ˜‚ğ—®ğ—´ğ—² ğ— ğ—¼ğ—±ğ—²ğ—¹ (ğ—™ğ—¿ğ—¼ğ—º ğ—¦ğ—°ğ—¿ğ—®ğ˜ğ—°ğ—µ) - Sebastian Raschka, [Book](https://lnkd.in/ddyVSfTu) ,  [Code](https://lnkd.in/d9uE345p)



